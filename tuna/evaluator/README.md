# Tuna Evaluator
Tuna evaluator is designed for LLM evaluation through **GPT-4** (or **GPT-3.5-turbo**).

## Brief
The proposed tool enable users to provide a json question dataset adhering to our predefined criteria. This dataset will then be employed to generate responses from specific Language Model (LLM) instances. Subsequently, the tool will categorize these generated responses into either pair-wise or single-wise evaluations, which will be submitted to ChatGPT for assessment.

In the context of pair-wise evaluation, the comparison will be made between two model outputs to ascertain the better one, while single-wise evaluation involves judging the merit of each specific response in isolation.

## Dataset
Dataset should be json array with same format as tuna, except that `content` field for assistant should be blank.(placeholder for answer generation). This tool enables **multi-turn** evaluation, so more than one round of conversation is acceptable.
```json
[
    {
        "messages": [
            {
                "content": "[Optional Background Context]",
                "role": "system"
            },
            {
                "content": "Could you write a captivating short story beginning with the sentence: The old abandoned house at the end of the street held a secret that no one had ever discovered.",
                "role": "user"
            },
            {
                "content": "[TO BE GENERATED BY LLM]",
                "role": "assistant"
            },
            {
                "content": "Now, do the same task again but only use four-word sentences.",
                "role": "user"
            },
            {
                "content": "[TO BE GENERATED BY LLM]",
                "role": "assistant"
            }
        ]
    }
]
```

## Usage
1. Set ENV-VARs:
    - `OPENAI_KEY` Your openai secret key;
    - `OPENAI_BASE` (Optional) API Base for OPENAI Official Server, default to `https://api.openai.com`
2. Exec 
```
python3 ./evaluate.py \
--tuna-api-addresses 34.147.2.221:8082^llama-2@gpt-3.5-turbo,34.147.2.221:8082^llama-3@text-davinci-003 \
--dataset ./sample.json \
--output-dir ./
```

- `tuna-api-addresses`: Register model with host address, alias name(unqiue), and model name: `host^model_alias@model_name`.(Separated by `,` for multiple registration);
- `dataset`: Input dataset path;
- `output-dir`: Output path for evaluation;
- `add-gpt-3`: Add GPT3.5 for comparision.
- `add-gpt-4`: Add GPT4 for comparision.
- `evaluator`: Evaluator to use. Choices: `['gpt-4', 'gpt-3.5-turbo-16k']`